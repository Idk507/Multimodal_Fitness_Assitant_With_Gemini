{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import speech_recognition as sr\n",
    "\n",
    "# LangChain \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, llms\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.schema import Generation, LLMResult, SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.llms.base import BaseLLM\n",
    "\n",
    "# Translator for multilingual \n",
    "from googletrans import Translator\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = \"\"\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"Warning: GOOGLE_API_KEY is not set.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleGenAI(LLM):\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"google-generativeai\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.candidates[0].content.parts[0].text\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        return self._call(prompt, stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression for parsing actions\n",
    "action_re = re.compile(r'^Action:\\s*(\\w+)\\s*:\\s*(.+)$')\n",
    "\n",
    "# Chatbot class for text-only multi-turn interaction\n",
    "class Chatbot:\n",
    "    def __init__(self, system):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "\n",
    "    def execute(self):\n",
    "        # Prepare prompt from the conversation history\n",
    "        prompt = \"\\n\".join([f'{msg[\"role\"]}:{msg[\"content\"]}' for msg in self.messages])\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        raw_response = model.generate_content(prompt)\n",
    "        # Extract text content from the first candidate\n",
    "        result_text = raw_response.candidates[0].content.parts[0].text\n",
    "        return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_llm = GoogleGenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert medical doctor with extensive experience in diagnosing and treating patient queries. Your role is to engage patients in a structured, empathetic, and conversational flow to identify the root cause of their health issues—such as illnesses, aches, or discomforts—and provide tailored solutions. These solutions should include appropriate remedies, medication suggestions (with general disclaimers if needed), lifestyle advice, diet plans, and exercise recommendations to promote overall health and recovery.\n",
    "\n",
    "For example:\n",
    "\n",
    "Patient: \"I’ve been having a fever.\"\n",
    "Doctor: \"How long have you been experiencing this fever?\"\n",
    "Patient: \"For three days.\"\n",
    "Doctor: \"Are you also experiencing symptoms like a dry cough, runny nose, or body aches?\"\n",
    "Patient: \"No, doctor.\"\n",
    "Doctor: \"Alright, let’s narrow this down. Have you noticed any other symptoms, like chills, fatigue, or loss of appetite? Also, have you been able to measure your temperature?\"\n",
    "Your responses should:\n",
    "\n",
    "Ask follow-up questions to gather more details about symptoms, duration, severity, and any relevant medical history.\n",
    "Analyze the information provided to suggest possible causes (e.g., viral infection, bacterial issue, etc.).\n",
    "Recommend remedies (e.g., rest, hydration), over-the-counter medications if appropriate (e.g., paracetamol for fever), and caution about consulting a healthcare provider for prescriptions or severe cases.\n",
    "Provide a simple, practical diet plan to support recovery (e.g., light meals, hydration tips) and address irregular eating habits for long-term health.\n",
    "Suggest basic exercises or wellness tips (e.g., light stretching or breathing exercises) suitable for their condition.\n",
    "Maintain a professional yet caring tone, ensuring the patient feels heard and supported.\n",
    "If information is missing, politely prompt the patient to clarify. Avoid making definitive diagnoses requiring in-person tests, and include disclaimers like ‘Please consult a local doctor if symptoms persist or worsen.’ Ensure all advice aligns with general medical knowledge and promotes a healthy lifestyle.\"\"\".strip()\n",
    "\n",
    "# Action functions using the Gemini API\n",
    "def generate_workout(level):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(f\"Generate a workout plan for a {level} fitness level\")\n",
    "    return response.candidates[0].content.parts[0].text \n",
    "\n",
    "def suggest_meal(preferences):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(f\"Suggest a meal plan with {preferences}\")\n",
    "    return response.candidates[0].content.parts[0].text\n",
    "\n",
    "\n",
    "# Mapping actions to functions\n",
    "known_actions = {\n",
    "    \"generate_workout\": generate_workout,\n",
    "    \"suggest_meal\": suggest_meal,\n",
    "    \n",
    "}\n",
    "\n",
    "# Function for text-based multi-turn conversation\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Chatbot(system_prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(\"\\nAssistant response:\")\n",
    "        print(result)\n",
    "        # Look for an action command in the response\n",
    "        actions = [action_re.match(a) for a in result.split('\\n') if action_re.match(a)]\n",
    "        if actions:\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(f\"Unknown action: {action}: {action_input}\")\n",
    "            print(f\"\\n-- running action: {action} with input: {action_input}\")\n",
    "            observation = known_actions[action](action_input.strip())\n",
    "            print(f\"\\nObservation: {observation}\\n\")\n",
    "            next_prompt = f\"Answer: {observation}\"\n",
    "            # Pause briefly before the next turn\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            # No further action, so end conversation\n",
    "            return\n",
    "\n",
    "# Function to capture voice input from the microphone and return recognized text\n",
    "def get_voice_query():\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    print(\"Please speak your query now...\")\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        voice_text = recognizer.recognize_google(audio)\n",
    "        print(f\"Recognized voice query: {voice_text}\")\n",
    "        return voice_text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, could not understand the audio.\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from the speech recognition service; {e}\")\n",
    "    return None\n",
    "\n",
    "# Function to handle image queries.\n",
    "# The user provides a path to an image and an optional accompanying text query.\n",
    "def handle_image_query():\n",
    "    image_path = input(\"Enter the path to the image file: \").strip()\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(\"File not found. Please check the path and try again.\")\n",
    "        return\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image.show()  # This will open the image using the default image viewer\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image: {e}\")\n",
    "        return\n",
    "\n",
    "    user_query = input(\"Enter your query related to this image: \").strip()\n",
    "    # Upload the file to Google Generative AI (if supported)\n",
    "    try:\n",
    "        sample_file = genai.upload_file(path=image_path, display_name=\"Image\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading image: {e}\")\n",
    "        return\n",
    "\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")\n",
    "    # Create a prompt that includes both the query and the reference to the uploaded image\n",
    "    prompt = (f\"Using the uploaded image and the query '{user_query}', provide fitness-related advice or suggestions. \"\n",
    "              \"The image shows a fitness-related activity or equipment. Assist the user with workout tips, form correction, \"\n",
    "              \"or equipment usage guidance.\")\n",
    "    response = model.generate_content([sample_file, prompt])\n",
    "    print(\"\\nAssistant response for image query:\")\n",
    "    print(response.text)\n",
    "\n",
    "# Main function to choose the query modality\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"\\nChoose query mode:\")\n",
    "        print(\"1 - Text query\")\n",
    "        print(\"2 - Voice query\")\n",
    "        print(\"3 - Image query\")\n",
    "        print(\"q - Quit\")\n",
    "        choice = input(\"Enter your choice: \").strip().lower()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            user_query = input(\"Enter your text query: \").strip()\n",
    "            query(user_query)\n",
    "        elif choice == \"2\":\n",
    "            voice_query = get_voice_query()\n",
    "            if voice_query:\n",
    "                query(voice_query)\n",
    "        elif choice == \"3\":\n",
    "            handle_image_query()\n",
    "        elif choice == \"q\":\n",
    "            print(\"Exiting application.\")\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select 1, 2, 3 or q.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
